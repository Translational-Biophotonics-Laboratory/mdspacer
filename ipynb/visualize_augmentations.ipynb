{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "420b0395",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\.virtualenvs\\BoneSegmentation-dpCXn3BL\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEVICE: cuda:0\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from torch.utils.data import DataLoader\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from os.path import abspath, join, dirname\n",
    "from tqdm import tqdm\n",
    "import sklearn\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(\"..\", \"unet\")))\n",
    "from dataset import COCODataset\n",
    "\n",
    "WIDTH = 256\n",
    "HEIGHT = 256\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"DEVICE: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862964ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = \"D:\\\\Datasets\\\\Segmentation\\\\COCO\\\\images\\\\val2017\"\n",
    "mask_path = \"D:\\\\Datasets\\\\Segmentation\\\\COCO\\\\masks\\\\val2017\"\n",
    "\n",
    "params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": True,\n",
    "    \"num_workers\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1054d21",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [00:12,  1.03s/it]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "classes should include all valid labels that can be in y",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     22\u001b[0m cocoloader \u001b[38;5;241m=\u001b[39m DataLoader(cocodata, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Mean: tensor([0.4690, 0.4456, 0.4062]), Std: tensor([0.2752, 0.2701, 0.2847])\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# mean, std = cocodata.get_dataset_mean_and_std(cocoloader)\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# print(f\"Mean: {mean}, Std: {std}\")\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mcocodata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_class_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcocoloader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(class_weights)\n",
      "File \u001b[1;32m~\\BoneSegmentation\\unet\\dataset.py:66\u001b[0m, in \u001b[0;36mCOCODataset.get_class_weights\u001b[1;34m(self, dataloader)\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m12\u001b[39m: \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     65\u001b[0m     y \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((y, torch\u001b[38;5;241m.\u001b[39mflatten(y_batch)))\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasses\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.virtualenvs\\BoneSegmentation-dpCXn3BL\\lib\\site-packages\\sklearn\\utils\\class_weight.py:43\u001b[0m, in \u001b[0;36mcompute_class_weight\u001b[1;34m(class_weight, classes, y)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(y) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mset\u001b[39m(classes):\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses should include all valid labels that can be in y\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(class_weight) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# uniform class weights\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     weight \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones(classes\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: classes should include all valid labels that can be in y"
     ]
    }
   ],
   "source": [
    "transform = A.Compose(\n",
    "    [\n",
    "#         A.Resize(height=HEIGHT, width=WIDTH),\n",
    "        A.Resize(height=int(HEIGHT*1.13), width=int(WIDTH*1.13)),\n",
    "        A.RandomCrop(height=HEIGHT, width=WIDTH),\n",
    "        A.Rotate(limit=90, p=0.9),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.1),\n",
    "        A.CLAHE(clip_limit=3.0, p=0.35),\n",
    "        A.ColorJitter(p=0.3),\n",
    "        A.GaussNoise(p=0.3),\n",
    "        A.Normalize(\n",
    "            mean=[0.4690, 0.4456, 0.4062],\n",
    "            std=[0.2752, 0.2701, 0.2847],\n",
    "            max_pixel_value=255.0\n",
    "        ),\n",
    "        ToTensorV2()\n",
    "    ]\n",
    ")\n",
    "\n",
    "cocodata = COCODataset(image_path, mask_path, transform=transform)\n",
    "cocoloader = DataLoader(cocodata, **params) \n",
    "# Mean: tensor([0.4690, 0.4456, 0.4062]), Std: tensor([0.2752, 0.2701, 0.2847])\n",
    "# mean, std = cocodata.get_dataset_mean_and_std(cocoloader)\n",
    "# print(f\"Mean: {mean}, Std: {std}\")\n",
    "\n",
    "# class_weights = cocodata.get_class_weights(cocoloader)\n",
    "# print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79420c1a",
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# data (n_batch, 3, height, width), target (n_batch, height, width)\n",
    "data, targets = next(iter(cocoloader))\n",
    "\n",
    "print(\"Targets Shape: \", targets.shape)\n",
    "print(\"Data Shape: \", data.shape)\n",
    "\n",
    "\n",
    "ax_size = BATCH_SIZE**0.5\n",
    "assert ax_size.is_integer() # make sure batch size perfect square\n",
    "ax_size = int(ax_size)\n",
    "\n",
    "f, axarr = plt.subplots(ax_size, ax_size, figsize=(25,25))\n",
    "count = 0\n",
    "for i in range(ax_size):\n",
    "    for j in range(ax_size):\n",
    "        image = data[count]\n",
    "        print(image.shape)\n",
    "        fig_img = torch.moveaxis(image, 0, -1)\n",
    "        axarr[i,j].imshow(fig_img)\n",
    "        plt.axis(\"off\")\n",
    "        print(\n",
    "            f\"\"\"\\nimage shape: {image.dtype},\n",
    "            \\n\\tmean: {torch.mean(image, dim=[1,2])},\n",
    "            \\n\\tstd: {torch.std(image, dim=[1,2])}\"\"\"\n",
    "        )\n",
    "        count += 1\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BoneSegmentationEnv",
   "language": "python",
   "name": "bonesegmentationenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
